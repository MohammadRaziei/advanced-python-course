{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6d928",
   "metadata": {},
   "source": [
    "# 🧠 Introduction to Hazm for Natural Language Processing (NLP)\n",
    "\n",
    "## 📘 What Is NLP?\n",
    "\n",
    "**Natural Language Processing (NLP)** is a field of Artificial Intelligence that enables machines to **understand, interpret, and generate human language**.\n",
    "\n",
    "It powers many everyday applications:\n",
    "- 🧑‍💬 Chatbots and virtual assistants\n",
    "- 🔍 Search engines\n",
    "- 📄 Text classification (e.g. spam detection)\n",
    "- 🗣️ Sentiment and emotion analysis\n",
    "- 📰 News summarization\n",
    "- 🧠 Machine translation\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 Why Python Is Ideal for NLP\n",
    "\n",
    "Python is the **de facto language** for NLP because of:\n",
    "- A vast ecosystem of libraries (`NLTK`, `spaCy`, `transformers`, ...)\n",
    "- Easy integration with machine learning frameworks\n",
    "- Clean syntax for text manipulation\n",
    "\n",
    "---\n",
    "\n",
    "## 🌍 NLP for Persian Language\n",
    "\n",
    "Most global NLP tools focus on English or multilingual models.\n",
    "\n",
    "However, Persian (Farsi) requires:\n",
    "- Proper handling of **right-to-left (RTL)** text\n",
    "- Normalization of Arabic-based script\n",
    "- Tokenization with Persian-specific rules\n",
    "\n",
    "This is where **Hazm** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What Is Hazm?\n",
    "\n",
    "**[Hazm](https://github.com/sobhe/hazm)** is a Python library for processing Persian (Farsi) text.  \n",
    "It provides basic tools for:\n",
    "- Normalization\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- POS tagging\n",
    "- Sentence segmentation\n",
    "\n",
    "### 📦 Installation\n",
    "\n",
    "```bash\n",
    "pip install hazm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Example: Using Hazm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9fdaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: کتاب‌های خوبی برای یادگیری زبان فارسی نوشته شده‌اند.\n",
      "Tokens: ['کتاب\\u200cهای', 'خوبی', 'برای', 'یادگیری', 'زبان', 'فارسی', 'نوشته', 'شده\\u200cاند', '.']\n",
      "کتاب‌های → Stem: کتاب, Lemma: کتاب\n",
      "خوبی → Stem: خوب, Lemma: خوبی\n",
      "برای → Stem: برا, Lemma: برای\n",
      "یادگیری → Stem: یادگیر, Lemma: یادگیری\n",
      "زبان → Stem: زب, Lemma: زبان\n",
      "فارسی → Stem: فارس, Lemma: فارسی\n",
      "نوشته → Stem: نوشته, Lemma: نوشته\n",
      "شده‌اند → Stem: شده‌اند, Lemma: شد#شو\n",
      ". → Stem: ., Lemma: .\n"
     ]
    }
   ],
   "source": [
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "text = \"کتاب‌های خوبی برای یادگیری زبان فارسی نوشته شده‌اند.\"\n",
    "\n",
    "# Normalize text\n",
    "normalized = normalizer.normalize(text)\n",
    "print(\"Normalized:\", normalized)\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(normalized)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Stem and Lemmatize\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"{token} → Stem: {stemmer.stem(token)}, Lemma: {lemmatizer.lemmatize(token)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3793a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What Hazm Does\n",
    "\n",
    "| Feature            | Description                                                   |\n",
    "|--------------------|---------------------------------------------------------------|\n",
    "| `Normalizer`       | Standardizes Persian script (e.g. removes diacritics, fixes spaces) |\n",
    "| `word_tokenize()`  | Splits text into words                                        |\n",
    "| `sentence_tokenize()` | Splits text into sentences                                 |\n",
    "| `Stemmer`          | Strips suffixes (e.g. ها، تر، ترین)                            |\n",
    "| `Lemmatizer`       | Converts words to dictionary form                             |\n",
    "| `POSTagger`        | Assigns grammatical labels to each word (requires model)      |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- **NLP** allows machines to understand human language\n",
    "- **Python** offers a powerful NLP toolkit with easy syntax\n",
    "- For **Persian**, **Hazm** is a dedicated library providing essential preprocessing tools\n",
    "- Hazm enables proper normalization, stemming, tokenization, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9a19a",
   "metadata": {},
   "source": [
    "# 🌐 Extracting Text from the Web for NLP — Fast HTML Parsing with `selectolax`\n",
    "\n",
    "## 📡 NLP Needs Real-World Data — And It’s on the Web\n",
    "\n",
    "As Natural Language Processing (NLP) continues to grow, so does the need for large and diverse **textual data**.\n",
    "\n",
    "Thanks to the rise of the internet:\n",
    "- 💬 Blog posts\n",
    "- 📰 News articles\n",
    "- 📚 Open books\n",
    "- 🛒 Reviews and product descriptions\n",
    "- 💼 Social media posts\n",
    "\n",
    "All provide rich text sources for training language models, sentiment classifiers, and more.\n",
    "\n",
    "> ⚠️ Most of this data is not structured — it's embedded inside raw **HTML**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What Is a Parser?\n",
    "\n",
    "A **parser** is a tool that reads and understands a document’s structure — like HTML — and lets you extract specific content from it.\n",
    "\n",
    "For example:\n",
    "- Extract the **title** of a webpage\n",
    "- Find all **links** in an article\n",
    "- Grab a product's **price** or **description**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Popular HTML Parsers in Python\n",
    "\n",
    "| Parser         | Description                                 |\n",
    "|----------------|---------------------------------------------|\n",
    "| `BeautifulSoup`| The most popular, easy-to-use parser         |\n",
    "| `lxml`         | Extremely fast, C-based parser               |\n",
    "| `html5lib`     | Standards-compliant but slow                 |\n",
    "| `selectolax`   | Lightweight, blazing fast (Rust-based) ✅    |\n",
    "| `parsel`       | Used in Scrapy for XPath and CSS selectors   |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Why Use `selectolax`?\n",
    "\n",
    "**Selectolax** is one of the **fastest HTML parsers in Python**, written in **Rust** under the hood.\n",
    "\n",
    "Here’s a benchmark from [this article on Medium](https://python.plainenglish.io/8-most-popular-python-html-web-scraping-packages-with-benchmarks-bfef9179dbf8):\n",
    "\n",
    "![Selectolax is the fastest parser](https://miro.medium.com/v2/resize:fit:875/0*Rxw_xuQ8_k6VlXyZ.png)\n",
    "\n",
    "> ✅ As you can see, **selectolax is 10–20× faster** than traditional parsers like `BeautifulSoup`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Example: Parsing a Web Page with Selectolax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbccd655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sample Page\n",
      "Description: This is a sample paragraph.\n",
      "Links:\n",
      "- Page 1: https://example.com/page1\n",
      "- Page 2: https://example.com/page2\n"
     ]
    }
   ],
   "source": [
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to the Sample Page</h1>\n",
    "    <p class=\"description\">This is a sample paragraph.</p>\n",
    "    <ul>\n",
    "        <li><a href=\"https://example.com/page1\">Page 1</a></li>\n",
    "        <li><a href=\"https://example.com/page2\">Page 2</a></li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML content\n",
    "parser = HTMLParser(html_content)\n",
    "\n",
    "# Extract the title\n",
    "title = parser.css_first(\"title\").text() if parser.css_first(\"title\") else \"No Title Found\"\n",
    "print(f\"Title: {title}\")\n",
    "\n",
    "# Extract the description\n",
    "description = parser.css_first(\".description\").text() if parser.css_first(\".description\") else \"No Description Found\"\n",
    "print(f\"Description: {description}\")\n",
    "\n",
    "# Extract all links\n",
    "links = []\n",
    "for tag in parser.css(\"a\"):\n",
    "    link = tag.attributes.get(\"href\", \"\")\n",
    "    text = tag.text(strip=True)\n",
    "    links.append((text, link))\n",
    "\n",
    "print(\"Links:\")\n",
    "for text, url in links:\n",
    "    print(f\"- {text}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b778f76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Web scraping is a **crucial step** in building NLP datasets\n",
    "- A **parser** helps extract clean, structured text from messy HTML\n",
    "- `selectolax` offers:\n",
    "  - 🚀 Unmatched speed (Rust backend)\n",
    "  - 🧼 Clean CSS selector syntax\n",
    "  - 🧩 Easy integration with NLP pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cafd24",
   "metadata": {},
   "source": [
    "# 🌐 Real-World Web Scraping Example: Wikipedia Table Extraction\n",
    "\n",
    "When working with NLP or data science, we often need structured data from websites like **Wikipedia**.\n",
    "\n",
    "Let’s go through two different approaches to extract a table from a Wikipedia page about *Mobile Communications of Iran*:\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Method 1: Manual HTML Parsing with `selectolax`\n",
    "\n",
    "Here, we fetch the raw HTML, parse it, find the table manually, and convert it to a `pandas` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b0edf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Native name               شرکت ارتباطات سیار ایران (همراه اول)\n",
      "0          Company type                                       Semi-private\n",
      "1             Traded as                       TSE:HMRZ1ISIN:  IRO1HMRZ0007\n",
      "2              Industry  .mw-parser-output .plainlist ol,.mw-parser-out...\n",
      "3               Founded                           1992; 33 years ago(1992)\n",
      "4          Headquarters                                        Tehran,Iran\n",
      "5           Area served                                               Iran\n",
      "6            Key people  Mehdi Akhavan Behabadi(CEO)Babak Tarakomeh(Mem...\n",
      "7              Products  Fixed-lineandmobile telephony,Internetservices...\n",
      "8                 Owner                            TCI(84.15%)Sukuk(5.91%)\n",
      "9   Number of employees                                              5000+\n",
      "10               Parent                                                TCI\n",
      "11         Subsidiaries                                           mobinnet\n",
      "12              Website                             www.mci.ir/web/en/home\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selectolax.parser import HTMLParser\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the webpage\n",
    "url = \"https://en.wikipedia.org/wiki/Mobile_Communications_of_Iran\"\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(\"Failed to load page\")\n",
    "html_content = response.text\n",
    "\n",
    "# Step 2: Parse the HTML\n",
    "parser = HTMLParser(html_content)\n",
    "\n",
    "# Step 3: Locate the infobox table\n",
    "tables = parser.css(\"table.infobox\")\n",
    "if not tables:\n",
    "    raise Exception(\"No tables found\")\n",
    "\n",
    "target_table = tables[0]\n",
    "\n",
    "# Step 4: Extract rows and cells\n",
    "rows = target_table.css(\"tr\")\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    cells = [cell.text(strip=True) for cell in row.css(\"th, td\")]\n",
    "    if cells and cells[0]:\n",
    "        data.append(cells)\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7172f",
   "metadata": {},
   "source": [
    "✅ **Pros:**\n",
    "- Full control over parsing structure\n",
    "- Lightweight and fast (Rust-based)\n",
    "\n",
    "❗ **Cons:**\n",
    "- Manual and verbose\n",
    "- Requires understanding of HTML structure\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Method 2: The One-Liner with `pandas.read_html`\n",
    "\n",
    "Now, here’s the same result using just **one line** with Pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7bcc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1           Native name               شرکت ارتباطات سیار ایران (همراه اول)\n",
      "0          Company type                                       Semi-private\n",
      "1             Traded as                      TSE: HMRZ1 ISIN: IRO1HMRZ0007\n",
      "2              Industry          TelecommunicationsMobile Network Operator\n",
      "3               Founded                                 1992; 33 years ago\n",
      "4          Headquarters                                       Tehran, Iran\n",
      "5           Area served                                               Iran\n",
      "6            Key people  Mehdi Akhavan Behabadi (CEO) Babak Tarakomeh (...\n",
      "7              Products  Fixed-line and mobile telephony, Internet serv...\n",
      "8                 Owner                          TCI (84.15%)Sukuk (5.91%)\n",
      "9   Number of employees                                              5000+\n",
      "10               Parent                                                TCI\n",
      "11         Subsidiaries                                           mobinnet\n",
      "12              Website                             www.mci.ir/web/en/home\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Mobile_Communications_of_Iran\"\n",
    "tables = pd.read_html(url, attrs={\"class\": \"infobox\"})  # Look for 'infobox' class tables\n",
    "\n",
    "df = tables[0].dropna()\n",
    "df.columns = df.iloc[0]\n",
    "df = df[1:].reset_index(drop=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbebac",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "✅ **Pros:**\n",
    "- Extremely simple\n",
    "- Automatically extracts all tables as DataFrames\n",
    "- Handles HTML parsing, table structure, and even rowspan/colspan\n",
    "\n",
    "❗ **Cons:**\n",
    "- Less customizable\n",
    "- Requires a relatively clean HTML structure\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Summary\n",
    "\n",
    "| Feature               | `selectolax`                   | `pandas.read_html`               |\n",
    "|------------------------|--------------------------------|----------------------------------|\n",
    "| Control                | ✅ Full control                 | 🔸 Minimal                       |\n",
    "| Speed                  | ✅ Very fast (Rust-based)       | 🔸 Moderate                      |\n",
    "| Simplicity             | 🔸 Manual steps                 | ✅ Super simple                  |\n",
    "| Ideal for              | Custom scraping & preprocessing| Ready-made structured tables     |\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 **Conclusion:**  \n",
    "> If your goal is **quickly loading structured tables**, use `pandas.read_html`.  \n",
    "> But if you need **custom parsing** or want to extract other elements (e.g., headings, links, nested tables), `selectolax` is the right tool.\n",
    "\n",
    "You now know **both the low-level and high-level way** to scrape tabular data from websites — great job! 🧠📄\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
