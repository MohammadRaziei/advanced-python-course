{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6d928",
   "metadata": {},
   "source": [
    "# ðŸ§  Introduction to Hazm for Natural Language Processing (NLP)\n",
    "\n",
    "## ðŸ“˜ What Is NLP?\n",
    "\n",
    "**Natural Language Processing (NLP)** is a field of Artificial Intelligence that enables machines to **understand, interpret, and generate human language**.\n",
    "\n",
    "It powers many everyday applications:\n",
    "- ðŸ§‘â€ðŸ’¬ Chatbots and virtual assistants\n",
    "- ðŸ” Search engines\n",
    "- ðŸ“„ Text classification (e.g. spam detection)\n",
    "- ðŸ—£ï¸ Sentiment and emotion analysis\n",
    "- ðŸ“° News summarization\n",
    "- ðŸ§  Machine translation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ Why Python Is Ideal for NLP\n",
    "\n",
    "Python is the **de facto language** for NLP because of:\n",
    "- A vast ecosystem of libraries (`NLTK`, `spaCy`, `transformers`, ...)\n",
    "- Easy integration with machine learning frameworks\n",
    "- Clean syntax for text manipulation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ NLP for Persian Language\n",
    "\n",
    "Most global NLP tools focus on English or multilingual models.\n",
    "\n",
    "However, Persian (Farsi) requires:\n",
    "- Proper handling of **right-to-left (RTL)** text\n",
    "- Normalization of Arabic-based script\n",
    "- Tokenization with Persian-specific rules\n",
    "\n",
    "This is where **Hazm** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ What Is Hazm?\n",
    "\n",
    "**[Hazm](https://github.com/sobhe/hazm)** is a Python library for processing Persian (Farsi) text.  \n",
    "It provides basic tools for:\n",
    "- Normalization\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- POS tagging\n",
    "- Sentence segmentation\n",
    "\n",
    "### ðŸ“¦ Installation\n",
    "\n",
    "```bash\n",
    "pip install hazm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Example: Using Hazm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9fdaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n",
      "Tokens: ['Ú©ØªØ§Ø¨\\u200cÙ‡Ø§ÛŒ', 'Ø®ÙˆØ¨ÛŒ', 'Ø¨Ø±Ø§ÛŒ', 'ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ', 'Ø²Ø¨Ø§Ù†', 'ÙØ§Ø±Ø³ÛŒ', 'Ù†ÙˆØ´ØªÙ‡', 'Ø´Ø¯Ù‡\\u200cØ§Ù†Ø¯', '.']\n",
      "Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ â†’ Stem: Ú©ØªØ§Ø¨, Lemma: Ú©ØªØ§Ø¨\n",
      "Ø®ÙˆØ¨ÛŒ â†’ Stem: Ø®ÙˆØ¨, Lemma: Ø®ÙˆØ¨ÛŒ\n",
      "Ø¨Ø±Ø§ÛŒ â†’ Stem: Ø¨Ø±Ø§, Lemma: Ø¨Ø±Ø§ÛŒ\n",
      "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ â†’ Stem: ÛŒØ§Ø¯Ú¯ÛŒØ±, Lemma: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ\n",
      "Ø²Ø¨Ø§Ù† â†’ Stem: Ø²Ø¨, Lemma: Ø²Ø¨Ø§Ù†\n",
      "ÙØ§Ø±Ø³ÛŒ â†’ Stem: ÙØ§Ø±Ø³, Lemma: ÙØ§Ø±Ø³ÛŒ\n",
      "Ù†ÙˆØ´ØªÙ‡ â†’ Stem: Ù†ÙˆØ´ØªÙ‡, Lemma: Ù†ÙˆØ´ØªÙ‡\n",
      "Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ â†’ Stem: Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯, Lemma: Ø´Ø¯#Ø´Ùˆ\n",
      ". â†’ Stem: ., Lemma: .\n"
     ]
    }
   ],
   "source": [
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "text = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\"\n",
    "\n",
    "# Normalize text\n",
    "normalized = normalizer.normalize(text)\n",
    "print(\"Normalized:\", normalized)\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(normalized)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Stem and Lemmatize\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"{token} â†’ Stem: {stemmer.stem(token)}, Lemma: {lemmatizer.lemmatize(token)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3793a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” What Hazm Does\n",
    "\n",
    "| Feature            | Description                                                   |\n",
    "|--------------------|---------------------------------------------------------------|\n",
    "| `Normalizer`       | Standardizes Persian script (e.g. removes diacritics, fixes spaces) |\n",
    "| `word_tokenize()`  | Splits text into words                                        |\n",
    "| `sentence_tokenize()` | Splits text into sentences                                 |\n",
    "| `Stemmer`          | Strips suffixes (e.g. Ù‡Ø§ØŒ ØªØ±ØŒ ØªØ±ÛŒÙ†)                            |\n",
    "| `Lemmatizer`       | Converts words to dictionary form                             |\n",
    "| `POSTagger`        | Assigns grammatical labels to each word (requires model)      |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- **NLP** allows machines to understand human language\n",
    "- **Python** offers a powerful NLP toolkit with easy syntax\n",
    "- For **Persian**, **Hazm** is a dedicated library providing essential preprocessing tools\n",
    "- Hazm enables proper normalization, stemming, tokenization, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9a19a",
   "metadata": {},
   "source": [
    "# ðŸŒ Extracting Text from the Web for NLP â€” Fast HTML Parsing with `selectolax`\n",
    "\n",
    "## ðŸ“¡ NLP Needs Real-World Data â€” And Itâ€™s on the Web\n",
    "\n",
    "As Natural Language Processing (NLP) continues to grow, so does the need for large and diverse **textual data**.\n",
    "\n",
    "Thanks to the rise of the internet:\n",
    "- ðŸ’¬ Blog posts\n",
    "- ðŸ“° News articles\n",
    "- ðŸ“š Open books\n",
    "- ðŸ›’ Reviews and product descriptions\n",
    "- ðŸ’¼ Social media posts\n",
    "\n",
    "All provide rich text sources for training language models, sentiment classifiers, and more.\n",
    "\n",
    "> âš ï¸ Most of this data is not structured â€” it's embedded inside raw **HTML**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ What Is a Parser?\n",
    "\n",
    "A **parser** is a tool that reads and understands a documentâ€™s structure â€” like HTML â€” and lets you extract specific content from it.\n",
    "\n",
    "For example:\n",
    "- Extract the **title** of a webpage\n",
    "- Find all **links** in an article\n",
    "- Grab a product's **price** or **description**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§° Popular HTML Parsers in Python\n",
    "\n",
    "| Parser         | Description                                 |\n",
    "|----------------|---------------------------------------------|\n",
    "| `BeautifulSoup`| The most popular, easy-to-use parser         |\n",
    "| `lxml`         | Extremely fast, C-based parser               |\n",
    "| `html5lib`     | Standards-compliant but slow                 |\n",
    "| `selectolax`   | Lightweight, blazing fast (Rust-based) âœ…    |\n",
    "| `parsel`       | Used in Scrapy for XPath and CSS selectors   |\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Why Use `selectolax`?\n",
    "\n",
    "**Selectolax** is one of the **fastest HTML parsers in Python**, written in **Rust** under the hood.\n",
    "\n",
    "Hereâ€™s a benchmark from [this article on Medium](https://python.plainenglish.io/8-most-popular-python-html-web-scraping-packages-with-benchmarks-bfef9179dbf8):\n",
    "\n",
    "![Selectolax is the fastest parser](https://miro.medium.com/v2/resize:fit:875/0*Rxw_xuQ8_k6VlXyZ.png)\n",
    "\n",
    "> âœ… As you can see, **selectolax is 10â€“20Ã— faster** than traditional parsers like `BeautifulSoup`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Example: Parsing a Web Page with Selectolax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbccd655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sample Page\n",
      "Description: This is a sample paragraph.\n",
      "Links:\n",
      "- Page 1: https://example.com/page1\n",
      "- Page 2: https://example.com/page2\n"
     ]
    }
   ],
   "source": [
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to the Sample Page</h1>\n",
    "    <p class=\"description\">This is a sample paragraph.</p>\n",
    "    <ul>\n",
    "        <li><a href=\"https://example.com/page1\">Page 1</a></li>\n",
    "        <li><a href=\"https://example.com/page2\">Page 2</a></li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML content\n",
    "parser = HTMLParser(html_content)\n",
    "\n",
    "# Extract the title\n",
    "title = parser.css_first(\"title\").text() if parser.css_first(\"title\") else \"No Title Found\"\n",
    "print(f\"Title: {title}\")\n",
    "\n",
    "# Extract the description\n",
    "description = parser.css_first(\".description\").text() if parser.css_first(\".description\") else \"No Description Found\"\n",
    "print(f\"Description: {description}\")\n",
    "\n",
    "# Extract all links\n",
    "links = []\n",
    "for tag in parser.css(\"a\"):\n",
    "    link = tag.attributes.get(\"href\", \"\")\n",
    "    text = tag.text(strip=True)\n",
    "    links.append((text, link))\n",
    "\n",
    "print(\"Links:\")\n",
    "for text, url in links:\n",
    "    print(f\"- {text}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b778f76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- Web scraping is a **crucial step** in building NLP datasets\n",
    "- A **parser** helps extract clean, structured text from messy HTML\n",
    "- `selectolax` offers:\n",
    "  - ðŸš€ Unmatched speed (Rust backend)\n",
    "  - ðŸ§¼ Clean CSS selector syntax\n",
    "  - ðŸ§© Easy integration with NLP pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cafd24",
   "metadata": {},
   "source": [
    "# ðŸŒ Real-World Web Scraping Example: Wikipedia Table Extraction\n",
    "\n",
    "When working with NLP or data science, we often need structured data from websites like **Wikipedia**.\n",
    "\n",
    "Letâ€™s go through two different approaches to extract a table from a Wikipedia page about *Mobile Communications of Iran*:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± Method 1: Manual HTML Parsing with `selectolax`\n",
    "\n",
    "Here, we fetch the raw HTML, parse it, find the table manually, and convert it to a `pandas` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b0edf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Native name               Ø´Ø±Ú©Øª Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ø³ÛŒØ§Ø± Ø§ÛŒØ±Ø§Ù† (Ù‡Ù…Ø±Ø§Ù‡ Ø§ÙˆÙ„)\n",
      "0          Company type                                       Semi-private\n",
      "1             Traded as                       TSE:HMRZ1ISIN:  IRO1HMRZ0007\n",
      "2              Industry  .mw-parser-output .plainlist ol,.mw-parser-out...\n",
      "3               Founded                           1992; 33Â years ago(1992)\n",
      "4          Headquarters                                        Tehran,Iran\n",
      "5           Area served                                               Iran\n",
      "6            Key people  Mehdi Akhavan Behabadi(CEO)Babak Tarakomeh(Mem...\n",
      "7              Products  Fixed-lineandmobile telephony,Internetservices...\n",
      "8                 Owner                            TCI(84.15%)Sukuk(5.91%)\n",
      "9   Number of employees                                              5000+\n",
      "10               Parent                                                TCI\n",
      "11         Subsidiaries                                           mobinnet\n",
      "12              Website                             www.mci.ir/web/en/home\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selectolax.parser import HTMLParser\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the webpage\n",
    "url = \"https://en.wikipedia.org/wiki/Mobile_Communications_of_Iran\"\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(\"Failed to load page\")\n",
    "html_content = response.text\n",
    "\n",
    "# Step 2: Parse the HTML\n",
    "parser = HTMLParser(html_content)\n",
    "\n",
    "# Step 3: Locate the infobox table\n",
    "tables = parser.css(\"table.infobox\")\n",
    "if not tables:\n",
    "    raise Exception(\"No tables found\")\n",
    "\n",
    "target_table = tables[0]\n",
    "\n",
    "# Step 4: Extract rows and cells\n",
    "rows = target_table.css(\"tr\")\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    cells = [cell.text(strip=True) for cell in row.css(\"th, td\")]\n",
    "    if cells and cells[0]:\n",
    "        data.append(cells)\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7172f",
   "metadata": {},
   "source": [
    "âœ… **Pros:**\n",
    "- Full control over parsing structure\n",
    "- Lightweight and fast (Rust-based)\n",
    "\n",
    "â— **Cons:**\n",
    "- Manual and verbose\n",
    "- Requires understanding of HTML structure\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Method 2: The One-Liner with `pandas.read_html`\n",
    "\n",
    "Now, hereâ€™s the same result using just **one line** with Pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7bcc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1           Native name               Ø´Ø±Ú©Øª Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ø³ÛŒØ§Ø± Ø§ÛŒØ±Ø§Ù† (Ù‡Ù…Ø±Ø§Ù‡ Ø§ÙˆÙ„)\n",
      "0          Company type                                       Semi-private\n",
      "1             Traded as                      TSE: HMRZ1 ISIN: IRO1HMRZ0007\n",
      "2              Industry          TelecommunicationsMobile Network Operator\n",
      "3               Founded                                 1992; 33Â years ago\n",
      "4          Headquarters                                       Tehran, Iran\n",
      "5           Area served                                               Iran\n",
      "6            Key people  Mehdi Akhavan Behabadi (CEO) Babak Tarakomeh (...\n",
      "7              Products  Fixed-line and mobile telephony, Internet serv...\n",
      "8                 Owner                          TCI (84.15%)Sukuk (5.91%)\n",
      "9   Number of employees                                              5000+\n",
      "10               Parent                                                TCI\n",
      "11         Subsidiaries                                           mobinnet\n",
      "12              Website                             www.mci.ir/web/en/home\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Mobile_Communications_of_Iran\"\n",
    "tables = pd.read_html(url, attrs={\"class\": \"infobox\"})  # Look for 'infobox' class tables\n",
    "\n",
    "df = tables[0].dropna()\n",
    "df.columns = df.iloc[0]\n",
    "df = df[1:].reset_index(drop=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbebac",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… **Pros:**\n",
    "- Extremely simple\n",
    "- Automatically extracts all tables as DataFrames\n",
    "- Handles HTML parsing, table structure, and even rowspan/colspan\n",
    "\n",
    "â— **Cons:**\n",
    "- Less customizable\n",
    "- Requires a relatively clean HTML structure\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Summary\n",
    "\n",
    "| Feature               | `selectolax`                   | `pandas.read_html`               |\n",
    "|------------------------|--------------------------------|----------------------------------|\n",
    "| Control                | âœ… Full control                 | ðŸ”¸ Minimal                       |\n",
    "| Speed                  | âœ… Very fast (Rust-based)       | ðŸ”¸ Moderate                      |\n",
    "| Simplicity             | ðŸ”¸ Manual steps                 | âœ… Super simple                  |\n",
    "| Ideal for              | Custom scraping & preprocessing| Ready-made structured tables     |\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ’¡ **Conclusion:**  \n",
    "> If your goal is **quickly loading structured tables**, use `pandas.read_html`.  \n",
    "> But if you need **custom parsing** or want to extract other elements (e.g., headings, links, nested tables), `selectolax` is the right tool.\n",
    "\n",
    "You now know **both the low-level and high-level way** to scrape tabular data from websites â€” great job! ðŸ§ ðŸ“„\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
