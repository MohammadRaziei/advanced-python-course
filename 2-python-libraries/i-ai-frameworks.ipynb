{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ef9772",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Introduction to TensorFlow for Regression\n",
    "\n",
    "## üöÄ Popular AI Frameworks\n",
    "\n",
    "Today, some of the most widely used frameworks for building AI models include:\n",
    "\n",
    "- **TensorFlow** (by Google) ‚Äì used in research, production, and mobile\n",
    "- **PyTorch** (by Meta) ‚Äì great for research and dynamic development\n",
    "- **JAX** (by Google) ‚Äì powerful for numerical computation with automatic differentiation\n",
    "- **Keras** ‚Äì high-level API that runs on top of TensorFlow\n",
    "- **ONNX Runtime**, **MXNet**, **CNTK** ‚Äì other alternatives for deployment and research\n",
    "\n",
    "In this lesson, we'll use **TensorFlow** to solve a simple **regression problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What Is TensorFlow?\n",
    "\n",
    "**TensorFlow** is an open-source framework developed by Google for building and training machine learning models.\n",
    "\n",
    "- It uses **tensors** (multi-dimensional arrays) as the basic data structure.\n",
    "- It builds a **computational graph** that can be optimized, executed on CPU/GPU/TPU, and exported for production.\n",
    "- It supports both **eager execution** and **graph execution** (via `@tf.function` or Keras Functional API).\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Problem: Simple Regression\n",
    "\n",
    "We want to model the relationship \\( y = 2x \\) using just a few data points. This is a simple **supervised regression** task.\n",
    "\n",
    "Here‚Äôs the code that defines and trains a neural network using **TensorFlow‚Äôs Functional API**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495ecf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "Prediction for x = 10: 20.460192\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 1\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Define training data\n",
    "x = np.array([1, 2, 3, 4, 15], dtype=np.float32).reshape(-1, 1)\n",
    "y = np.array([2, 4, 6, 8, 30], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "# Define model using Functional API\n",
    "inputs = tf.keras.Input(shape=(1,))\n",
    "hidden1 = tf.keras.layers.Dense(16, activation=\"relu\")(inputs)\n",
    "hidden2 = tf.keras.layers.Dense(16, activation=\"relu\")(hidden1)\n",
    "hidden3 = tf.keras.layers.Dense(16, activation=\"relu\")(hidden2)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"linear\")(hidden3)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "model.fit(x, y, epochs=100, verbose=0)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(np.array([[10.0]]))\n",
    "print(\"Prediction for x = 10:\", prediction.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2dd46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Why Use the Functional API?\n",
    "\n",
    "TensorFlow provides two main ways to build models:\n",
    "\n",
    "1. **Sequential API** ‚Äì for simple stack-like models\n",
    "2. **Functional API** ‚Äì for models with complex architectures\n",
    "\n",
    "We use the **Functional API** here because:\n",
    "- It is more flexible.\n",
    "- It supports multiple inputs, multiple outputs, residual connections, etc.\n",
    "- It treats **layers as callables** (functions), enabling clean and modular model construction.\n",
    "\n",
    "```python\n",
    "output = layer(input)  # This is what \"callable structure\" means\n",
    "```\n",
    "\n",
    "Each layer behaves like a function:  \n",
    "‚û°Ô∏è It takes a tensor as input and returns a new tensor as output.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà TensorFlow Builds a Graph, Then Executes\n",
    "\n",
    "TensorFlow operates in two phases:\n",
    "\n",
    "1. **Graph Construction**  \n",
    "   When we define the model, TensorFlow **builds a computational graph** (a dataflow representation of operations).\n",
    "2. **Graph Execution**  \n",
    "   When we call `fit()`, TensorFlow **compiles and runs** the graph efficiently on hardware (CPU/GPU/TPU).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Benefits of Graph-Based Execution\n",
    "\n",
    "| Benefit                  | Description |\n",
    "|--------------------------|-------------|\n",
    "| üîÅ Optimized execution   | TensorFlow can merge, fuse, and parallelize operations for performance |\n",
    "| üì¶ Portability           | Graphs can be exported (`SavedModel`) and run anywhere |\n",
    "| üß™ Reproducibility       | Graphs are deterministic if seeded properly |\n",
    "| üöÄ Deployment-ready      | Graphs can be used in mobile, embedded, and production pipelines |\n",
    "| ‚ö° Hardware acceleration | TensorFlow executes graphs efficiently on GPUs and TPUs |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "- TensorFlow is a powerful graph-based framework used in both research and industry.\n",
    "- The **Functional API** allows clean and flexible model design using a **callable layer structure**.\n",
    "- TensorFlow builds and compiles a graph before execution, enabling optimization, portability, and performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc69c51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7992dc7d",
   "metadata": {},
   "source": [
    "# üîÑ PyTorch: A Popular Alternative to TensorFlow\n",
    "\n",
    "While TensorFlow remains a dominant framework in production and deployment, **PyTorch** has become the **preferred choice for researchers and developers** ‚Äî especially in the academic and deep learning research community.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ù§Ô∏è Why PyTorch Is Loved by Many\n",
    "\n",
    "| Feature                 | PyTorch Advantage                                      |\n",
    "|--------------------------|--------------------------------------------------------|\n",
    "| üß† Dynamic computation    | Code behaves like regular Python ‚Äî no graph compiling |\n",
    "| üß™ Research-friendly      | Simple to debug, prototype, and test                  |\n",
    "| üß± Clean syntax           | Feels native to Python, no layers-as-objects abstraction |\n",
    "| üõ† Full control           | More transparent access to model internals            |\n",
    "| üöÄ Production-ready       | TorchScript + ONNX = deploy anywhere                  |\n",
    "\n",
    "PyTorch gives you the freedom to write **deep learning code as if it's regular Python** ‚Äî using loops, conditionals, and print statements with no surprises.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Equivalent Regression Example in PyTorch\n",
    "\n",
    "Below is the exact same **regression task** we built in TensorFlow ‚Äî but now implemented in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7f6c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2432ed49f5494192b3afafc7b3d248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 199.4047\n",
      "Epoch 10: Loss = 70.4203\n",
      "Epoch 20: Loss = 23.2770\n",
      "Epoch 30: Loss = 8.0391\n",
      "Epoch 40: Loss = 1.1393\n",
      "Epoch 50: Loss = 0.1913\n",
      "Epoch 60: Loss = 0.0379\n",
      "Epoch 70: Loss = 0.0121\n",
      "Epoch 80: Loss = 0.0051\n",
      "Epoch 90: Loss = 0.0024\n",
      "Prediction for x = 10: 19.998851776123047\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Training data\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0], [15.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0], [30.0]])\n",
    "\n",
    "# Define model using native Python class\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, 16)\n",
    "        self.fc2 = torch.nn.Linear(16, 16)\n",
    "        self.fc3 = torch.nn.Linear(16, 16)\n",
    "        self.fc4 = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Instantiate model, define loss and optimizer\n",
    "model = RegressionModel()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in trange(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Predict\n",
    "x_test = torch.tensor([[10.0]])\n",
    "prediction = model(x_test).item()\n",
    "print(\"Prediction for x = 10:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67de7fd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß† Why This Feels More ‚ÄúPythonic‚Äù\n",
    "\n",
    "Unlike TensorFlow's **graph-based model definition**, PyTorch lets you use:\n",
    "- Regular Python `class` syntax\n",
    "- Standard control flow (`if`, `for`)\n",
    "- Native `print()` for debugging\n",
    "- No explicit `compile()` or graph-tracing\n",
    "\n",
    "In essence, **you write neural networks like normal Python functions**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Aspect                        | TensorFlow                          | PyTorch                          |\n",
    "|-------------------------------|-------------------------------------|----------------------------------|\n",
    "| Style                         | Graph-based, declarative            | Eager, imperative                |\n",
    "| Syntax                        | Keras layers & APIs                 | Native Python classes & modules |\n",
    "| Debugging                     | Needs tools like `tf.print`         | Just use `print()`               |\n",
    "| Training                      | `fit()` abstraction                 | Manual training loop             |\n",
    "| Learning curve                | Slightly steeper                    | More intuitive for Python users  |\n",
    "\n",
    "---\n",
    "\n",
    "> üí¨ In modern ML practice, many developers use **both frameworks** ‚Äî TensorFlow for deployment, PyTorch for research. Choose the one that best fits your workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d380abd",
   "metadata": {},
   "source": [
    "## **CVXPY: The Optimization Backbone Behind AI Frameworks**  \n",
    "\n",
    "At the core of most **AI and deep learning frameworks** like TensorFlow and PyTorch lies **optimization**. Whether training a neural network, tuning hyperparameters, or even fine-tuning prompts in reinforcement learning, **optimization algorithms are the key drivers**.  \n",
    "\n",
    "However, sometimes we don't need a **full AI model**, but rather just **solving a direct optimization problem**. This is where **CVXPY** comes into play: a specialized **convex optimization** package in Python.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is CVXPY?**\n",
    "‚úÖ **A Python library for convex optimization**  \n",
    "‚úÖ **Solves linear, quadratic, and conic programs**  \n",
    "‚úÖ **Designed for optimization problems in engineering, finance, and machine learning**  \n",
    "\n",
    "üí° **Key Difference:** Unlike TensorFlow/PyTorch (which handle **gradient-based learning**), CVXPY is specialized for solving **well-defined convex optimization problems directly**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Solving a Continuous Convex Optimization Problem**  \n",
    "\n",
    "Let's solve a **simple convex optimization problem**:  \n",
    "\n",
    "$$\n",
    "\\min_{x} \\quad (x - 3)^2\n",
    "$$\n",
    "\n",
    "#### **CVXPY Code for a Simple Quadratic Optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92094082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal x: 3.0\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Define the optimization variable\n",
    "x = cp.Variable()\n",
    "\n",
    "# Define the objective function (minimize (x - 3)^2)\n",
    "objective = cp.Minimize((x - 3)**2)\n",
    "\n",
    "# Define the optimization problem\n",
    "problem = cp.Problem(objective)\n",
    "\n",
    "# Solve the problem\n",
    "problem.solve()\n",
    "\n",
    "# Print the optimal value of x\n",
    "print(\"Optimal x:\", x.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ae6d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Breaking Down the Key Components**  \n",
    "\n",
    "üîπ **`cp.Variable()`** ‚Üí Defines an **optimization variable** (unknown value to be optimized).  \n",
    "üîπ **`cp.Minimize((x - 3)**2)`** ‚Üí Defines a **convex objective function** to minimize.  \n",
    "üîπ **`cp.Problem(objective)`** ‚Üí Forms the optimization problem.  \n",
    "üîπ **`problem.solve()`** ‚Üí Runs the solver to find the **optimal value of x**.\n",
    "\n",
    "In this case, since we are minimizing \\((x-3)^2\\), the solution is **\\(x=3\\)**, which is expected.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is CVXPY Focused on Convex Problems?**  \n",
    "\n",
    "**Convex problems** are optimization problems where:\n",
    "1. The **objective function** is **convex** (e.g., quadratic, exponential, logarithmic functions).  \n",
    "2. The **constraints** form a **convex set** (e.g., linear inequalities).  \n",
    "\n",
    "These problems are **easier to solve** because:\n",
    "- **They have a unique global minimum** (no local minima traps like in deep learning).  \n",
    "- **Efficient solvers exist** (like interior-point methods).  \n",
    "\n",
    "For more on **convex optimization**, check out this resource:  \n",
    "üìå [Convex Optimization by Stephen Boyd](https://web.stanford.edu/~boyd/cvxbook/)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why Does This Matter in AI?**\n",
    "Even in AI and deep learning, many sub-problems are actually **convex optimization problems**:\n",
    "- **SVMs (Support Vector Machines)** are formulated as **convex quadratic programs**.  \n",
    "- **Lasso Regression** uses **L1-regularized convex optimization**.  \n",
    "- **Portfolio optimization** in finance is solved using **convex programming**.\n",
    "\n",
    "CVXPY allows **solving these problems directly** rather than relying on **gradient-based deep learning frameworks**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054515a",
   "metadata": {},
   "source": [
    "### **Constrained Optimization with CVXPY**  \n",
    "\n",
    "In many real-world scenarios, optimization problems are subject to **constraints**. These constraints define the feasible region where the optimization must occur. CVXPY is particularly powerful because it **naturally handles constrained convex optimization problems**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Constrained Optimization Problem**  \n",
    "\n",
    "Let's solve the following **constrained quadratic optimization problem**:\n",
    "\n",
    "$$\n",
    "\\min_{x} \\quad (x - 3)^2\n",
    "$$\n",
    "$$\n",
    "\\text{subject to} \\quad x \\geq 1\n",
    "$$\n",
    "\n",
    "Here, we are minimizing $(x - 3)^2$, but **with the constraint** that $ x $ must be **greater than or equal to 1**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95322dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal x: 3.0\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Define the optimization variable\n",
    "x = cp.Variable()\n",
    "\n",
    "# Define the objective function (minimize (x - 3)^2)\n",
    "objective = cp.Minimize((x - 3)**2)\n",
    "\n",
    "# Define the constraint (x ‚â• 1)\n",
    "constraint = [x >= 1]\n",
    "\n",
    "# Define the optimization problem\n",
    "problem = cp.Problem(objective, constraint)\n",
    "\n",
    "# Solve the problem\n",
    "problem.solve()\n",
    "\n",
    "# Print the optimal value of x\n",
    "print(\"Optimal x:\", x.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
